{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand written digits recognition with neural networks using Keras library\n",
    "## Introduction\n",
    "+ The following packages need to be installed:\n",
    " + The python packages `keras`, `tensorflow`, `numpy` and `matplotlib`.\n",
    " + Jupyter notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "+ Importing necessary modules : The cell underneath needs to be run before others to import modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils        import np_utils\n",
    "from keras.models       import Sequential\n",
    "from keras.layers       import Dense, Conv2D, MaxPooling2D, Activation, Dropout, Flatten, BatchNormalization\n",
    "from keras.datasets     import mnist\n",
    "from keras.optimizers   import Adam, Adadelta\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Useful functions to reshape the input and watch failed predictions from a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_1D():\n",
    "    # read data from dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_test_original = x_test\n",
    "    \n",
    "    # reshape x in 1D\n",
    "    x_train = x_train.reshape(x_train.shape[0], 784)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 784)\n",
    "\n",
    "    # cast x to float32 and normalize to [0,1]\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    # reshape y to 10-dim bit instead of int\n",
    "    y_test_original = y_test\n",
    "    y_train = np_utils.to_categorical(y_train, 10)\n",
    "    y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "    \n",
    "    \n",
    "def get_input_2D():\n",
    "    # read data from dataset\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # reshape in 2D, 28*28\n",
    "    x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "    #convert to float and normalize\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    # reshape y to 10-dim bit instead of int\n",
    "    y_train = np_utils.to_categorical(y_train, 10)\n",
    "    y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "def show_failed_cases(model, x_test, y_test, nb_to_show = 20):\n",
    "    l_1 = model.predict_classes(x_test)\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    l_2 = y_test\n",
    "    ll = [i for i in range(len(l_1)) if l_1[i] != l_2[i] ]\n",
    "    failed_sample = [ll[i] for i in random.sample(range(len(ll)), min(nb_to_show, len(ll)))]\n",
    "    \n",
    "    x_test = x_test.reshape(x_test.shape[0], 28, 28)\n",
    "    for fail in failed_sample:\n",
    "        plt.figure(fail)\n",
    "        img = x_test[fail]\n",
    "        print(\"real value is %s ; predicted value is %s\" % (y_test[fail], l_1[fail]))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "def show_history(history):\n",
    "    #  \"Accuracy\"\n",
    "    plt.figure(1)\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    #plt.legend(['train', 'validation'], loc='upper left')\n",
    "    #plt.show()\n",
    "    # \"Loss\"\n",
    "    plt.figure(2)\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    #plt.legend(['train', 'validation'], loc='upper left')\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first neural network : The multi-layer perceptron\n",
    "+ This first example uses only Dense (fully connected) layers. We used this NN to study the influence of some hyperparameters on an architecture of fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dense_layers(model, dense_layers_sizes, drop_out=None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(dense_layers_sizes[0], input_dim=784, activation='sigmoid'))\n",
    "    \n",
    "    for size in dense_layers_sizes[1:]:\n",
    "        if drop_out:\n",
    "            model.add(Dropout(drop_out))\n",
    "        model.add(Dense(size, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of dense layers :\n",
    "+ The first 2 or 3 dense layers do play an important part, but after, adding many layers do not really improve the prediction quality of the model, and has a big cost for the performance. Actually the best results are achieved with the smallest NN, made of only 1 hidden layer.\n",
    "+ It is interesting to see that if the NN is given too many dense layers, with random values to start with, it cannot \"learn\" anymore and keep predicting at a very low success rate (about 10%, as good as a random predictor). So it looks that too many layers, with no network structure, does not improve at all the model but diminishes its quality.\n",
    "+ Tests done on 4 epochs, batch_size 64, layer size of 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Influence of number of layers:\n",
    "print(\"Influence of number of dense layers :\")\n",
    "x_train, y_train, x_test, y_test = get_input_1D()\n",
    "layers = [256]\n",
    "for i in range(5):\n",
    "    print(\"With %s hidden layers\" % (i+1))\n",
    "    drop_out = 0.2\n",
    "    model = Sequential()\n",
    "    model = add_dense_layers(model, layers, drop_out)\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt=Adam(lr=0.01)\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                 optimizer = opt,\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "    hist = model.fit(x_train, y_train,\n",
    "             epochs = 8,\n",
    "             batch_size = 64,\n",
    "             verbose=0,\n",
    "             validation_data=(x_test, y_test))\n",
    "    \n",
    "    show_history(hist)\n",
    "    layers += [256]\n",
    "    loss, acc = model.evaluate(x_test, y_test, batch_size = 64)\n",
    "    print(\"accuracy : %s ; loss : %s\" % (acc, loss))\n",
    "    print(\"__________________________________\\n\")\n",
    "plt.figure(1)\n",
    "plt.legend([\"with %s hidden layers\" % (i+1) for i in range(6)], loc=4)\n",
    "plt.savefig(\"nb_layers_influence_on_acc.png\")\n",
    "plt.figure(2)\n",
    "plt.legend([\"with %s hidden layers\" % (i+1) for i in range(6)], loc=3)\n",
    "plt.savefig(\"nb_layers_influence_on_loss.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of dense layers :\n",
    "+ Playing with dense layer size does also have an influence on the results, here is a plot giving results according to different dense layer sizes.\n",
    "+ We can see that it is not really useful to go for gigantic dense layers, since they don't seem to provide better results than the smaller ones. In this case, we can see that after a layer of size 256, training a NN with a bigger layer does not improve the results, but demands more time to train the NN.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# influence of layer size :\n",
    "print(\"\\n\\nInfluence of layer size\")\n",
    "x_train, y_train, x_test, y_test = get_input_1D()\n",
    "layers = [16]\n",
    "for i in range(8):\n",
    "    print(\"Layer size : %s\\n\" % (16*2**i))\n",
    "    drop_out = 0.2\n",
    "    model = Sequential()\n",
    "    model = add_dense_layers(model, layers, drop_out)\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt=Adam(lr=0.01)\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                 optimizer = opt,\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "    hist = model.fit(x_train, y_train,\n",
    "             epochs = 6,\n",
    "             batch_size = 64,\n",
    "             verbose=0,\n",
    "             validation_data=(x_test, y_test))\n",
    "    \n",
    "    show_history(hist)\n",
    "    layers = [2*layers[0]]\n",
    "    loss, acc = model.evaluate(x_test, y_test, batch_size = 64)\n",
    "    print(\"accuracy : %s ; loss : %s\" % (acc, loss))\n",
    "    print(\"__________________________________\\n\")\n",
    "plt.figure(1)\n",
    "plt.legend([\"layer size : %s\" % (16*2**i) for i in range(6)], loc=4)\n",
    "plt.savefig(\"layer_size_influence_on_acc.png\")\n",
    "plt.figure(2)\n",
    "plt.savefig(\"layer_size_influence_on_loss.png\")\n",
    "plt.legend([\"layer size :%s\" % (16*2**i) for i in range(6)], loc=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of epochs : \n",
    "+ The NN does learn a bit during each epoch ; but this has limits as well, since we can see that a very high number of epochs does not improve the overall results or diminish the overall losses. We can clearly see that on this graph, where after a dozen epochs the NN stops improving and reaches some kind of limit.\n",
    "+ We can also see that after a number of epochs (in this case a dozen), the model starts overfitting, since the results obtained on the training data are getting better and better, while the results obtained on the validation data do not improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#influence of number of epochs\n",
    "print(\"Influence of number of epochs\")\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784, activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "opt=Adam(lr=0.01)\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = opt,\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_input_1D()\n",
    "score_acc, score_loss = [], []\n",
    "for i in range(50):\n",
    "    print(\"epoch %s\" % (i+1))\n",
    "    model.fit(x_train, y_train,\n",
    "             epochs = 1,\n",
    "             batch_size = 64,\n",
    "             verbose=0)\n",
    "    loss, acc = model.evaluate(x_test, y_test, batch_size = 64)\n",
    "    score_acc += [acc]\n",
    "    score_loss += [loss]\n",
    "    print(\"score : acc = %s ; loss = %s\" % (acc, loss))\n",
    "plt.plot(score_acc)\n",
    "plt.show()\n",
    "plt.plot(score_loss)\n",
    "plt.show()\n",
    "\n",
    "#show_failed_cases(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout influence :\n",
    "+ Including dropout layers will prevent overfitting, this feature is really an important one. For a NN to be well trained without any dropout, we obtain better results on the training samples than on the test samples, and vice versa when dropout is enabled. We can also see that the best values in our case for the dropout are around 0.2 / 0.3, meaning that around 20-30% of the cells are desactivated at each iteration.\n",
    "+ That's why all Dense layers will be added to the model along with a dropout layer to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overfitting\n",
    "\n",
    "x_train, y_train, x_test, y_test = get_input_1D()\n",
    "score_acc, score_loss = [], []\n",
    "for i in range(5):\n",
    "    print(\"Dropout value : %s\" % (0.1*i))\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_dim=784, activation='sigmoid'))\n",
    "    if i != 0:\n",
    "        model.add(Dropout(0.1*i))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt=Adam(lr=0.01)\n",
    "    model.compile(loss = 'categorical_crossentropy',\n",
    "                 optimizer = opt,\n",
    "                 metrics = ['accuracy'])\n",
    "\n",
    "    model.fit(x_train, y_train,\n",
    "             epochs = 8,\n",
    "             batch_size = 64)\n",
    "    loss, acc = model.evaluate(x_test, y_test, batch_size = 64)\n",
    "    score_acc += [acc]\n",
    "    score_loss += [loss]\n",
    "    print(\"score : acc = %s ; loss = %s\" % (acc, loss))\n",
    "plt.plot(score_acc)\n",
    "plt.show()\n",
    "plt.plot(loss_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In brief\n",
    "+ We see that a very simple MLP, made of only one hidden layer of size 256, along with a Dropout layer to prevent overfitting, does achieve decent results already. But to recognize images, one need to see features in these images, and that's when we start using convolutional NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another architecture : using convolutionnal NN\n",
    "+ In this part, we did use the following architecture :\n",
    "    + Conv2D\n",
    "    + Conv2D\n",
    "    + Dropout\n",
    "    + MaxPooling2D\n",
    "    + Flatten\n",
    "    + Dense\n",
    "    + Dense\n",
    "+ By running tests on the values to retain for hyperparameters, we obtained some NN achieving very high accuracy rates. Here is a description of those tests and the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments:\n",
    "\n",
    "+ Since running the experiment with all the compilation between parameters will take very long time, our procedure was to work on tuning parameters one by one. We change one parameter and fix the others then take the best results and change another parameter.\n",
    "+ The chosen order to tune paramterers was the following : kernel_size, batch_size, optimizer, loss_function, number_of_epoch. Number_of_epoch was chosen to be the last one so as to finish the experiment in the shortest possible time, running all the previous experiments with 4 epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel size\n",
    "+ We started with kernel_size. We chose 6 different sizes (3, 5, 7, 9, 11, 13). The results of a serie of runs are shown beneath.\n",
    "+ We can see that the best results are achieved with kernel of small size (3, 5 and 7)\n",
    "\n",
    "\n",
    "![influence of kernel size](img/kernel_size.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size\n",
    "+ Then came batch size : We tried with the values : 32, 64, 128, 256, 512, 1024. The results of a serie of runs are shown below.\n",
    "+ We get better results with batch size of 32 and 64 mainly. These values are big enough to compute a good gradient, and still let the NN train faster, and have a good amount of updates in an epoch.\n",
    "\n",
    "\n",
    "![influence of batch size](img/batch_size.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different optimizers\n",
    "+ We also tried different optimizers supplied with the keras library. Here are some results obtained with different optimizers.\n",
    "\n",
    "\n",
    "![influence of optimizer](img/optimizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing loss functions\n",
    "+ We then compared our results by changing the loss function. We then found out that much better results could be obtained using `binary_crossentropy`. \n",
    "+ Here is a table of the results obtained by testing different hyperparameters with the following loss functions : categorical_crossentropy, mean_squared_error, mean_absolute_error, categorical_hinge, binary_crossentropy, poisson\n",
    "\n",
    "![loss functions](img/loss_fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code\n",
    "+ Here is the code from which we ran all the tests to get good hyperparameters on convolutional NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import sys\n",
    "\n",
    "def solve(loss_function, optimizer, batch_size, kernel_size, epochs):\n",
    "\tnum_classes = 10\n",
    "\t# loss_function \t= sys.argv[1]\n",
    "\t# optimizer\t\t= sys.argv[2]\n",
    "\t# batch_size \t\t= int(sys.argv[3])\n",
    "\t# kernel_size\t\t= int(sys.argv[4])\n",
    "\t# epochs\t\t\t= int(sys.argv[5])\n",
    "\n",
    "\n",
    "\toptimizers = {\n",
    "\t\t'Adadelta'\t:\tkeras.optimizers.Adadelta(lr=.01),\n",
    "\t\t'Adam'\t\t:\tkeras.optimizers.Adam(lr=.01),\n",
    "\t\t'Adagrad' \t:\tkeras.optimizers.Adagrad(lr=0.01),\n",
    "\t\t'SGD' \t\t:\tkeras.optimizers.SGD(lr=0.01),\n",
    "\t\t'RMSprop' \t:\tkeras.optimizers.RMSprop(lr=0.01),\n",
    "\t\t'Adamax'\t:\tkeras.optimizers.Adamax(lr=.01),\n",
    "\t\t'Nadam'\t\t:\tkeras.optimizers.Nadam(lr=.01),\n",
    "\t}\n",
    "\n",
    "\n",
    "\n",
    "\t# input image dimensions\n",
    "\timg_rows, img_cols = 28, 28\n",
    "\n",
    "\t# the data, split between train and test sets\n",
    "\t(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "\tx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "\tx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\tinput_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "\tx_train = x_train.astype('float32')\n",
    "\tx_test = x_test.astype('float32')\n",
    "\tx_train /= 255\n",
    "\tx_test /= 255\n",
    "\tprint('x_train shape:', x_train.shape)\n",
    "\tprint(x_train.shape[0], 'train samples')\n",
    "\tprint(x_test.shape[0], 'test samples')\n",
    "\n",
    "\t# print (\"Bashar\")\n",
    "\t# convert class vectors to binary class matrices\n",
    "\ty_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "\ty_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Conv2D(32, kernel_size=(kernel_size, kernel_size),\n",
    "\t                 activation='relu',\n",
    "\t                 input_shape=input_shape))\n",
    "\n",
    "\tmodel.add(Conv2D(64, (kernel_size, kernel_size), activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.25))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\tmodel.compile(loss=loss_function,\n",
    "\t              optimizer=keras.optimizers.Adadelta(),\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "\tmodel.fit(x_train, y_train,\n",
    "\t          batch_size=batch_size,\n",
    "\t          epochs=epochs,\n",
    "\t          verbose=1,\n",
    "\t          validation_data=(x_test, y_test))\n",
    "\n",
    "\tscore = model.evaluate(x_test, y_test, verbose=0)\n",
    "\t# # model.summary()\n",
    "\n",
    "\tprint('Test loss:', score[0])\n",
    "\tprint('Test accuracy:', score[1])\n",
    "\n",
    "\treturn [score[0], score[1]]\n",
    "\n",
    "\n",
    "file = open(\"parameter.csv\", \"r\")\n",
    "line = '1'\n",
    "while line:\n",
    "\tresults_file = open(\"results.csv\", \"a\") \t\n",
    "\n",
    "\tline = file.readline()\n",
    "\tresults_file.write(line.replace('\\n','') + \",\")\n",
    "\tparams = line.split(',')\n",
    "\t\n",
    "\t[loss, accuracy] = solve(params[0], params[1], int(params[2]), int(params[3]), int(params[4]))\n",
    "\n",
    "\tresults_file.write(str(loss)+\",\")\n",
    "\tresults_file.write(str(accuracy) + \"\\n\")\n",
    "\n",
    "\tresults_file.close()\n",
    "\n",
    "# solve(\"categorical_crossentropy\", \"Adadelta\", 128, 3, 12)\n",
    "\n",
    "#params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using data augmentation techniques\n",
    "+ The keras library is really convenient to use, and data augmentation can be made in just a couple of lines using this library. Here is an example of data augmentation done, with decently chose parameters (little rotations, no flippping/mirroring ...)\n",
    "+ This is another feature preventing overfitting, since with it the training samples are different on each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "937/937 [==============================] - 741s 791ms/step - loss: 0.0504 - acc: 0.9824 - val_loss: 0.0070 - val_acc: 0.9974\n",
      "Epoch 2/5\n",
      "937/937 [==============================] - 749s 800ms/step - loss: 0.0211 - acc: 0.9929 - val_loss: 0.0047 - val_acc: 0.9983\n",
      "Epoch 3/5\n",
      "937/937 [==============================] - 739s 788ms/step - loss: 0.0167 - acc: 0.9945 - val_loss: 0.0047 - val_acc: 0.9984\n",
      "Epoch 4/5\n",
      "937/937 [==============================] - 741s 791ms/step - loss: 0.0142 - acc: 0.9953 - val_loss: 0.0038 - val_acc: 0.9986\n",
      "Epoch 5/5\n",
      "542/937 [================>.............] - ETA: 5:16 - loss: 0.0130 - acc: 0.9959"
     ]
    }
   ],
   "source": [
    "# read data from dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape in 2D, 28*28\n",
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "\n",
    "#convert to float and normalize\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# reshape y to 10-dim bit instead of int\n",
    "y_train = np_utils.to_categorical(y_train, 10)\n",
    "y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "\n",
    "gen = ImageDataGenerator(rotation_range=9, width_shift_range=0.1, shear_range=0.22,\n",
    "                         height_shift_range=0.1, zoom_range=0.15)\n",
    "\n",
    "test_gen = ImageDataGenerator()\n",
    "\n",
    "train_generator = gen.flow(x_train, y_train, batch_size=64)\n",
    "test_generator = test_gen.flow(x_test, y_test, batch_size=64)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    " \n",
    "model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "opt = Adam()\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = opt,\n",
    "             metrics = ['accuracy'])\n",
    "model.fit_generator(train_generator, steps_per_epoch=60000//64, epochs=5, \n",
    "                    validation_data=test_generator, validation_steps=10000//64)\n",
    "score = model.evaluate(x_test, y_test, batch_size = 64)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_failed_cases(model, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
